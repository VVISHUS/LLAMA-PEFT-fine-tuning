# ğŸ¦™ğŸ”§ Unsloth Llama-3 LoRA Fine-Tuning â€” Self-Learning Project

This project demonstrates how to **fine-tune a Llama-3 model** using **Unsloth**, **LoRA** (or QLoRA), and the **`trl` libraryâ€™s SFTTrainer** on a small, open instruction-following dataset.  
Itâ€™s designed as a **self-study pipeline** for understanding **parameter-efficient fine-tuning** of modern LLMs.

---

## ğŸ“Œ What Youâ€™ll Learn

âœ… How to:
- Load and quantize large LLMs (4-bit)  
- Apply LoRA (Low-Rank Adaptation)  
- Use instruction-style chat templates  
- Use `trl`â€™s **SFTTrainer** for supervised fine-tuning  
- Run inference with your fine-tuned model

---

## âš¡ï¸ Technologies Used

- [Unsloth](https://github.com/unslothai/unsloth) â€” Fast PEFT for Llama-family models  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)  
- [TRL](https://github.com/huggingface/trl) â€” for SFT and RLHF pipelines  
- [ğŸ¤— Datasets](https://huggingface.co/docs/datasets) â€” load and process dataset  
- [WandB](https://wandb.ai) *(optional)* â€” track experiments

---

## ğŸ“‚ Dataset Used

This example uses [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) â€” a compact, diverse instruction-following dataset based on ShareGPT-like conversational pairs.

---

## ğŸ“Œ Prerequisite

- You will rewuire wandb api to track logging
- Get it from - [WandB](https://wandb.ai) 

## âš¡ï¸ Next Steps:
- Will implement PPO using trl library
- Which includes training Reward Model first



